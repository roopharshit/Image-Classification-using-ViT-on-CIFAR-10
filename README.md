# Image-Classification-using-ViT-on-CIFAR-10


▪ Implemented a Vision Transformer (ViT) model using PyTorch on the CIFAR-10 dataset, achieving sophisticated image classification into 10 distinct categories through self-attention mechanisms.
▪ Developed a dual-phase training approach with an initial training phase on 45,000 images for comprehensive feature extraction, followed by fine-tuning on 5,000 images to optimize model generalization and performance.
▪ Applied advanced data augmentation techniques, including random cropping, horizontal flipping, rotation, color jittering, and random affine transformations, to increase dataset variability and robustness of the model.
▪ Utilized dropout layers and OneCycle learning rate scheduling for effective regularization and optimization, achieving an accuracy of 96.5%, while managing GPU resources efficiently using TAMU HPRC clusters for computational tasks.
